{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Importing Necessary Libraries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing necessary libraries\n",
    "import torch  # Import the PyTorch library for tensor computation\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer  # Import GPT-2 model and tokenizer from the transformers library\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "torch: A library for tensor computations, essential for deep learning tasks.\n",
    "\n",
    "transformers: Provides pre-trained models and tokenizers, specifically for working with the GPT-2 model in this case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Loading the Model and Tokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a larger model and tokenizer from the Hugging Face library\n",
    "model_name = 'gpt2-medium'  # You can switch to 'gpt2-large' for even better quality\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_name)  # Load the tokenizer for the selected model\n",
    "model = GPT2LMHeadModel.from_pretrained(model_name)  # Load the GPT-2 model for text generation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "model_name: Specifies which GPT-2 model to load (here, gpt2-medium).\n",
    "\n",
    "tokenizer: Converts text to token IDs that the model can understand.\n",
    "\n",
    "model: Loads the GPT-2 model for text generation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Setting the Device for Computation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2LMHeadModel(\n",
       "  (transformer): GPT2Model(\n",
       "    (wte): Embedding(50257, 1024)\n",
       "    (wpe): Embedding(1024, 1024)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0-23): 24 x GPT2Block(\n",
       "        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2SdpaAttention(\n",
       "          (c_attn): Conv1D(nf=3072, nx=1024)\n",
       "          (c_proj): Conv1D(nf=1024, nx=1024)\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D(nf=4096, nx=1024)\n",
       "          (c_proj): Conv1D(nf=1024, nx=4096)\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=1024, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set the device for computation\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")  # Use GPU if available, else fallback to CPU\n",
    "model.to(device)  # Move the model to the selected device (GPU or CPU)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "device: Checks if a GPU is available for faster computation and falls back to the CPU if not.\n",
    "\n",
    "model.to(device): Transfers the model to the selected device for processing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Getting Keyword Input from the User\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get keyword input from the user\n",
    "keyword = input(\"Enter your keyword: \")  # Prompt the user to enter a keyword for text generation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "input(): Prompts the user to enter a keyword that will guide the text generation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Function to Generate Text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to generate text based on the keyword\n",
    "def generate_text(keyword, max_length=150):  # Function takes a keyword and a maximum length for the output\n",
    "    # Create a contextual input sentence to guide the model\n",
    "    input_text = f\"Please provide interesting facts, health benefits, and recipes involving {keyword}: \"\n",
    "    input_ids = tokenizer.encode(input_text, return_tensors='pt').to(device)  # Encode the input text to tensor format and move to device\n",
    "\n",
    "    # Set attention mask (to ignore padding tokens)\n",
    "    attention_mask = torch.ones(input_ids.shape, device=device)  # Create an attention mask of ones (indicating all tokens should be attended to)\n",
    "\n",
    "    # Generate text with the model\n",
    "    with torch.no_grad():  # Disable gradient calculation for efficiency\n",
    "        output = model.generate(\n",
    "            input_ids,  # The input tensor for generation\n",
    "            max_length=max_length,  # Set the maximum length for the generated output\n",
    "            num_return_sequences=1,  # Number of sequences to generate\n",
    "            attention_mask=attention_mask,  # Provide the attention mask\n",
    "            pad_token_id=tokenizer.eos_token_id,  # Set pad token ID to end of sentence token\n",
    "            temperature=0.9,  # Control randomness in the output; lower values yield more predictable results\n",
    "            top_p=0.95,       # Nucleus sampling for diversity in generated text\n",
    "            do_sample=True    # Enable sampling for varied outputs\n",
    "        )\n",
    "\n",
    "    # Convert the generated tensor tokens back to text\n",
    "    generated_text = tokenizer.decode(output[0], skip_special_tokens=True)  # Decode the first generated sequence while skipping special tokens\n",
    "    return generated_text  # Return the generated text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "generate_text function:\n",
    "\n",
    "Takes a keyword and the maximum length of generated text as parameters.\n",
    "\n",
    "Prepares a prompt that guides the model's generation.\n",
    "\n",
    "Encodes the input prompt into tensors for processing.\n",
    "\n",
    "Sets an attention mask to indicate which tokens to focus on.\n",
    "\n",
    "Generates the text using the model with specified parameters for randomness and diversity.\n",
    "\n",
    "Decodes the generated output back to readable text and returns it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. Generating and Printing the Text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generated Text:\n",
      " Please provide interesting facts, health benefits, and recipes involving orange:  Please give us some examples (e.g., orange candy recipe)  to show how your favorite orange candy has improved or improved your health. Please do not hesitate to add any information you have that would be helpful. Thank you!\n",
      "This post was written by a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for sites to earn advertising fees by advertising and linking to Amazon.com.\n"
     ]
    }
   ],
   "source": [
    "# Generate and print the text based on the user's input keyword\n",
    "generated_text = generate_text(keyword)  # Call the function to generate text\n",
    "print(\"\\nGenerated Text:\\n\", generated_text)  # Print the generated text to the console\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "generate_text(keyword): Calls the function defined earlier to generate text based on the user’s input.\n",
    "\n",
    "print(): Displays the generated text in the console."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
