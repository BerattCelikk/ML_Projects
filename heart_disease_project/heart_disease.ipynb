{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   age  sex  cp  trestbps  chol  fbs  restecg  thalach  exang  oldpeak  slope  \\\n",
      "0   52    1   0       125   212    0        1      168      0      1.0      2   \n",
      "1   53    1   0       140   203    1        0      155      1      3.1      0   \n",
      "2   70    1   0       145   174    0        1      125      1      2.6      0   \n",
      "3   61    1   0       148   203    0        1      161      0      0.0      2   \n",
      "4   62    0   0       138   294    1        1      106      0      1.9      1   \n",
      "\n",
      "   ca  thal  target  \n",
      "0   2     3       0  \n",
      "1   0     3       0  \n",
      "2   0     3       0  \n",
      "3   1     3       0  \n",
      "4   3     2       0  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "# Reading the heart disease dataset\n",
    "data = pd.read_csv(\"heart.csv\")\n",
    "\n",
    "# Displaying the first five rows of the dataset\n",
    "print(data.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code Explanation\n",
    "\n",
    "import pandas as pd: This line imports the Pandas library and allows us to use it with the alias pd. Pandas is a powerful library for data manipulation and analysis, particularly with structured data like CSV files.\n",
    "\n",
    "data = pd.read_csv(\"heart.csv\"): This line reads the CSV file named heart.csv and stores it in the variable data. The dataset contains information related to heart disease, with various attributes for each patient.\n",
    "\n",
    "print(data.head()): This line prints the first five rows of the dataset, providing a quick overview of the data structure and the values contained within it. The output includes the following columns:\n",
    "\n",
    "age: The age of the patient.\n",
    "\n",
    "sex: The gender of the patient (1 = male, 0 = female).\n",
    "\n",
    "cp: Chest pain type (0-3).\n",
    "\n",
    "trestbps: Resting blood pressure (in mm Hg).\n",
    "\n",
    "chol: Serum cholesterol in mg/dl.\n",
    "\n",
    "fbs: Fasting blood sugar > 120 mg/dl (1 = true, 0 = false).\n",
    "\n",
    "restecg: Resting electrocardiographic results (0-2).\n",
    "\n",
    "thalach: Maximum heart rate achieved.\n",
    "\n",
    "exang: Exercise induced angina (1 = yes, 0 = no).\n",
    "\n",
    "oldpeak: ST depression induced by exercise relative to rest.\n",
    "\n",
    "slope: Slope of the peak exercise ST segment (0-2).\n",
    "\n",
    "ca: Number of major vessels (0-3) colored by fluoroscopy.\n",
    "\n",
    "thal: Thalassemia (1-3).\n",
    "\n",
    "target: Diagnosis of heart disease (1 = presence, 0 = absence)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the dataset:  (1025, 14)\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1025 entries, 0 to 1024\n",
      "Data columns (total 14 columns):\n",
      " #   Column    Non-Null Count  Dtype  \n",
      "---  ------    --------------  -----  \n",
      " 0   age       1025 non-null   int64  \n",
      " 1   sex       1025 non-null   int64  \n",
      " 2   cp        1025 non-null   int64  \n",
      " 3   trestbps  1025 non-null   int64  \n",
      " 4   chol      1025 non-null   int64  \n",
      " 5   fbs       1025 non-null   int64  \n",
      " 6   restecg   1025 non-null   int64  \n",
      " 7   thalach   1025 non-null   int64  \n",
      " 8   exang     1025 non-null   int64  \n",
      " 9   oldpeak   1025 non-null   float64\n",
      " 10  slope     1025 non-null   int64  \n",
      " 11  ca        1025 non-null   int64  \n",
      " 12  thal      1025 non-null   int64  \n",
      " 13  target    1025 non-null   int64  \n",
      "dtypes: float64(1), int64(13)\n",
      "memory usage: 112.2 KB\n",
      "None\n",
      "   age  sex  cp  trestbps  chol  fbs  restecg  thalach  exang  oldpeak  slope  \\\n",
      "0   52    1   0       125   212    0        1      168      0      1.0      2   \n",
      "1   53    1   0       140   203    1        0      155      1      3.1      0   \n",
      "2   70    1   0       145   174    0        1      125      1      2.6      0   \n",
      "3   61    1   0       148   203    0        1      161      0      0.0      2   \n",
      "4   62    0   0       138   294    1        1      106      0      1.9      1   \n",
      "\n",
      "   ca  thal  target  \n",
      "0   2     3       0  \n",
      "1   0     3       0  \n",
      "2   0     3       0  \n",
      "3   1     3       0  \n",
      "4   3     2       0  \n",
      "               age          sex           cp     trestbps        chol  \\\n",
      "count  1025.000000  1025.000000  1025.000000  1025.000000  1025.00000   \n",
      "mean     54.434146     0.695610     0.942439   131.611707   246.00000   \n",
      "std       9.072290     0.460373     1.029641    17.516718    51.59251   \n",
      "min      29.000000     0.000000     0.000000    94.000000   126.00000   \n",
      "25%      48.000000     0.000000     0.000000   120.000000   211.00000   \n",
      "50%      56.000000     1.000000     1.000000   130.000000   240.00000   \n",
      "75%      61.000000     1.000000     2.000000   140.000000   275.00000   \n",
      "max      77.000000     1.000000     3.000000   200.000000   564.00000   \n",
      "\n",
      "               fbs      restecg      thalach        exang      oldpeak  \\\n",
      "count  1025.000000  1025.000000  1025.000000  1025.000000  1025.000000   \n",
      "mean      0.149268     0.529756   149.114146     0.336585     1.071512   \n",
      "std       0.356527     0.527878    23.005724     0.472772     1.175053   \n",
      "min       0.000000     0.000000    71.000000     0.000000     0.000000   \n",
      "25%       0.000000     0.000000   132.000000     0.000000     0.000000   \n",
      "50%       0.000000     1.000000   152.000000     0.000000     0.800000   \n",
      "75%       0.000000     1.000000   166.000000     1.000000     1.800000   \n",
      "max       1.000000     2.000000   202.000000     1.000000     6.200000   \n",
      "\n",
      "             slope           ca         thal       target  \n",
      "count  1025.000000  1025.000000  1025.000000  1025.000000  \n",
      "mean      1.385366     0.754146     2.323902     0.513171  \n",
      "std       0.617755     1.030798     0.620660     0.500070  \n",
      "min       0.000000     0.000000     0.000000     0.000000  \n",
      "25%       1.000000     0.000000     2.000000     0.000000  \n",
      "50%       1.000000     0.000000     2.000000     1.000000  \n",
      "75%       2.000000     1.000000     3.000000     1.000000  \n",
      "max       2.000000     4.000000     3.000000     1.000000  \n"
     ]
    }
   ],
   "source": [
    "print(\"Shape of the dataset: \", data.shape)\n",
    "# Displaying the shape of the dataset (number of rows and columns)\n",
    "print(data.info())\n",
    "# Displaying a summary of the dataset, including data types and non-null counts\n",
    "print(data.head())\n",
    "# Displaying the first five rows of the dataset\n",
    "print(data.describe())\n",
    "# Providing descriptive statistics for the dataset, including count, mean, std, min, 25%, 50%, 75%, and max values\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code Explanation\n",
    "\n",
    "print(\"Shape of the dataset: \", data.shape): This line prints the shape of the dataset, which indicates that there are 1025 rows and 14 columns. This gives a quick overview of how many records (patients) and features (attributes) are present in the dataset.\n",
    "\n",
    "print(data.info()): This line outputs a summary of the dataset. It shows:\n",
    "\n",
    "The total number of entries (1025) and the range of the index (0 to 1024).\n",
    "A breakdown of each column, including the count of non-null entries, the data type of each column (e.g., int64, float64), and the total number of columns (14).\n",
    "This information helps identify data types and check for any missing values.\n",
    "print(data.head()): This line prints the first five rows of the dataset again, showing a sample of the data. The displayed columns include:\n",
    "\n",
    "age, sex, cp, trestbps, chol, fbs, restecg, thalach, exang, oldpeak, slope, ca, thal, and target. This output provides insight into the actual values of each feature for the first five patients.\n",
    "print(data.describe()): This line provides descriptive statistics for the dataset, including:\n",
    "\n",
    "Count: The number of non-null entries for each column.\n",
    "\n",
    "Mean: The average value of each column.\n",
    "\n",
    "Standard deviation (std): A measure of the variation in the dataset.\n",
    "\n",
    "Min: The minimum value.\n",
    "\n",
    "25%, 50%, and 75%: The quartiles of the data.\n",
    "\n",
    "Max: The maximum value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of training set: (820, 13)\n",
      "Size of test set: (205, 13)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Splitting features (X) and target variable (y)\n",
    "X = data.drop(\"target\", axis=1)  # Dropping the target variable\n",
    "y = data[\"target\"]                 # Target variable\n",
    "\n",
    "# Splitting into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "print(\"Size of training set:\", X_train.shape)\n",
    "# Printing the size of the training set (number of samples and features)\n",
    "print(\"Size of test set:\", X_test.shape)\n",
    "# Printing the size of the test set (number of samples and features)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code Explanation\n",
    "\n",
    "from sklearn.model_selection import train_test_split: This line imports the train_test_split function from the sklearn.model_selection module. This function is used to split the dataset into training and testing subsets.\n",
    "\n",
    "X = data.drop(\"target\", axis=1): This line creates a new DataFrame X by dropping the target column from the original dataset data. The target column represents the labels we want to predict, while X contains the features used for prediction.\n",
    "\n",
    "y = data[\"target\"]: This line assigns the target column from the dataset to the variable y. This variable will hold the labels corresponding to the features in X.\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42): This line splits the features and target variable into training and testing sets using the train_test_split function. The parameters are:\n",
    "\n",
    "X: Features DataFrame.\n",
    "\n",
    "y: Target variable.\n",
    "\n",
    "test_size=0.2: This specifies that 20% of the data should be allocated for testing, while 80% will be used for training.\n",
    "random_state=42: This ensures reproducibility by setting a seed for the random number generator.\n",
    "print(\"Size of training set:\", X_train.shape): This line prints the shape of the training set, which indicates it contains 820 samples and 13 features (the number of input variables).\n",
    "\n",
    "print(\"Size of test set:\", X_test.shape): This line prints the shape of the test set, which shows it contains 205 samples and 13 features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix:\n",
      " [[73 29]\n",
      " [13 90]]\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.72      0.78       102\n",
      "           1       0.76      0.87      0.81       103\n",
      "\n",
      "    accuracy                           0.80       205\n",
      "   macro avg       0.80      0.79      0.79       205\n",
      "weighted avg       0.80      0.80      0.79       205\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# Creating the Logistic Regression model\n",
    "model = LogisticRegression(max_iter=1000)\n",
    "\n",
    "# Fitting the model to the training data\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Making predictions on the test set\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Printing the confusion matrix\n",
    "print(\"Confusion matrix:\\n\", confusion_matrix(y_test, y_pred))\n",
    "# Printing the classification report\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code Explanation\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression: This line imports the LogisticRegression class from the sklearn.linear_model module. Logistic Regression is a popular classification algorithm used for binary classification tasks.\n",
    "\n",
    "from sklearn.metrics import classification_report, confusion_matrix: This line imports two metrics:\n",
    "\n",
    "confusion_matrix: Used to evaluate the accuracy of a classification.\n",
    "classification_report: Generates a report that includes precision, recall, F1-score, and support for each class.\n",
    "model = LogisticRegression(max_iter=1000): This line creates an instance of the LogisticRegression model with a maximum iteration limit of 1000. This is important to ensure the model converges during training.\n",
    "\n",
    "model.fit(X_train, y_train): This line trains the Logistic Regression model using the training data X_train and the target labels y_train. The model learns the relationship between the features and the target variable.\n",
    "\n",
    "y_pred = model.predict(X_test): This line uses the trained model to make predictions on the test set X_test. The predicted labels are stored in the variable y_pred.\n",
    "\n",
    "print(\"Confusion matrix:\\n\", confusion_matrix(y_test, y_pred)): This line calculates and prints the confusion matrix comparing the true labels y_test and the predicted labels y_pred. The confusion matrix provides insights into how many predictions were correct and how many were incorrect. It shows:\n",
    "\n",
    "True Negatives (TN): 73\n",
    "\n",
    "False Positives (FP): 29\n",
    "\n",
    "False Negatives (FN): 13\n",
    "\n",
    "True Positives (TP): 90\n",
    "\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred)): This line prints a classification report summarizing the precision, recall, F1-score, and support for each class:\n",
    "\n",
    "Precision: The ratio of correctly predicted positive observations to the total predicted positives.\n",
    "\n",
    "Recall (Sensitivity): The ratio of correctly predicted positive observations to all actual positives.\n",
    "\n",
    "F1-score: The weighted average of Precision and Recall. It provides a balance between both metrics.\n",
    "\n",
    "Support: The number of actual occurrences of the class in the specified dataset.\n",
    "\n",
    "The output shows:\n",
    "\n",
    "Class 0 (Negative class):\n",
    "\n",
    "Precision: 0.85\n",
    "\n",
    "Recall: 0.72\n",
    "\n",
    "F1-score: 0.78\n",
    "\n",
    "Support: 102\n",
    "\n",
    "Class 1 (Positive class):\n",
    "\n",
    "Precision: 0.76\n",
    "\n",
    "Recall: 0.87\n",
    "\n",
    "F1-score: 0.81\n",
    "\n",
    "Support: 103\n",
    "\n",
    "Overall accuracy: 0.80\n",
    "\n",
    "Macro average and weighted average values provide an overall performance measure across all classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix:\n",
      " [[73 29]\n",
      " [13 90]]\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.72      0.78       102\n",
      "           1       0.76      0.87      0.81       103\n",
      "\n",
      "    accuracy                           0.80       205\n",
      "   macro avg       0.80      0.79      0.79       205\n",
      "weighted avg       0.80      0.80      0.79       205\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Splitting features (X) and target variable (y)\n",
    "X = data.drop(\"target\", axis=1)  # Dropping the target variable\n",
    "y = data[\"target\"]                 # Target variable\n",
    "\n",
    "# Splitting into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Scaling the data\n",
    "scaler = StandardScaler()           # Creating an instance of StandardScaler\n",
    "X_train_scaled = scaler.fit_transform(X_train)  # Fitting and transforming the training data\n",
    "X_test_scaled = scaler.transform(X_test)        # Transforming the test data\n",
    "\n",
    "# Define and train the model\n",
    "model = LogisticRegression(max_iter=2000)  # Creating a Logistic Regression model with 2000 iterations\n",
    "model.fit(X_train_scaled, y_train)          # Fitting the model to the scaled training data\n",
    "\n",
    "# Making predictions\n",
    "y_pred = model.predict(X_test_scaled)      # Predicting the target variable for the test set\n",
    "\n",
    "# Printing results\n",
    "print(\"Confusion matrix:\\n\", confusion_matrix(y_test, y_pred))\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code Explanation\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression: This line imports the LogisticRegression class from the sklearn.linear_model module, allowing you to use the Logistic Regression algorithm for classification tasks.\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler: This line imports the StandardScaler class, which is used to standardize the features by removing the mean and scaling to unit variance.\n",
    "\n",
    "from sklearn.model_selection import train_test_split: This line imports the train_test_split function, which is used to split the dataset into training and testing sets.\n",
    "\n",
    "X = data.drop(\"target\", axis=1): This line creates the feature set X by dropping the target variable from the dataset data.\n",
    "\n",
    "y = data[\"target\"]: This line assigns the target variable y from the dataset.\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42): This line splits the data into training and testing sets, with 20% of the data allocated for testing. The random_state parameter ensures that the split is reproducible.\n",
    "\n",
    "scaler = StandardScaler(): This line creates an instance of the StandardScaler, which will be used to standardize the features.\n",
    "\n",
    "X_train_scaled = scaler.fit_transform(X_train): This line fits the StandardScaler to the training data and transforms it, resulting in scaled training data.\n",
    "\n",
    "X_test_scaled = scaler.transform(X_test): This line transforms the test data using the same scaler, ensuring that the test data is scaled based on the training data's statistics.\n",
    "\n",
    "model = LogisticRegression(max_iter=2000): This line creates a Logistic Regression model instance with a maximum of 2000 iterations to ensure convergence during training.\n",
    "\n",
    "model.fit(X_train_scaled, y_train): This line fits the Logistic Regression model to the scaled training data.\n",
    "\n",
    "y_pred = model.predict(X_test_scaled): This line makes predictions on the scaled test data using the trained model, storing the predicted values in y_pred.\n",
    "\n",
    "print(\"Confusion matrix:\\n\", confusion_matrix(y_test, y_pred)): This line prints the confusion matrix, which shows the performance of the classification model:\n",
    "\n",
    "True Negatives (TN): 73\n",
    "\n",
    "False Positives (FP): 29\n",
    "\n",
    "False Negatives (FN): 13\n",
    "\n",
    "True Positives (TP): 90\n",
    "\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred)): This line prints a detailed classification report, which includes:\n",
    "\n",
    "Precision: The ratio of correctly predicted positive observations to the total predicted positives.\n",
    "\n",
    "Recall: The ratio of correctly predicted positive observations to all actual positives.\n",
    "\n",
    "F1-score: The weighted average of Precision and Recall.\n",
    "\n",
    "Support: The number of actual occurrences of the class in the specified dataset.\n",
    "\n",
    "The output shows:\n",
    "\n",
    "Class 0 (Negative class):\n",
    "\n",
    "Precision: 0.85\n",
    "\n",
    "Recall: 0.72\n",
    "\n",
    "F1-score: 0.78\n",
    "\n",
    "Support: 102\n",
    "\n",
    "Class 1 (Positive class):\n",
    "\n",
    "Precision: 0.76\n",
    "\n",
    "Recall: 0.87\n",
    "\n",
    "F1-score: 0.81\n",
    "\n",
    "Support: 103\n",
    "\n",
    "Overall accuracy: 0.80\n",
    "\n",
    "Macro average and weighted average values provide an overall performance measure across all classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyperparameters: {'logisticregression__C': 0.1, 'logisticregression__penalty': 'l1', 'logisticregression__solver': 'liblinear'}\n",
      "Best accuracy: 0.87375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\berat\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "# Creating a synthetic dataset (you can use your own dataset)\n",
    "X, y = make_classification(n_samples=1000, n_features=20, random_state=42)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Creating a pipeline for data preprocessing and model training\n",
    "pipeline = make_pipeline(StandardScaler(), LogisticRegression(max_iter=500))\n",
    "\n",
    "# Hyperparameter grid\n",
    "param_grid = {\n",
    "    'logisticregression__C': [0.001, 0.01, 0.1, 1, 10, 100],\n",
    "    'logisticregression__penalty': ['l1', 'l2'],\n",
    "    'logisticregression__solver': ['liblinear', 'saga']  # Trying different solvers\n",
    "}\n",
    "\n",
    "# Defining GridSearchCV\n",
    "grid_search = GridSearchCV(pipeline, param_grid, cv=5, scoring='accuracy')\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Best hyperparameters and accuracy\n",
    "print(\"Best hyperparameters:\", grid_search.best_params_)\n",
    "print(\"Best accuracy:\", grid_search.best_score_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code Explanation\n",
    "\n",
    "from sklearn.datasets import make_classification: This line imports the make_classification function, which is used to generate a synthetic dataset for classification.\n",
    "\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV: This line imports the train_test_split function to split the dataset into training and testing sets, and GridSearchCV to perform hyperparameter tuning.\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression: This imports the LogisticRegression class, which will be used to create a logistic regression model.\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler: This imports the StandardScaler class, which standardizes features by removing the mean and scaling to unit variance.\n",
    "\n",
    "from sklearn.pipeline import make_pipeline: This imports the make_pipeline function to create a pipeline that combines multiple steps into one.\n",
    "\n",
    "X, y = make_classification(n_samples=1000, n_features=20, random_state=42): This line generates a synthetic dataset with 1000 samples and 20 features, assigning the features to X and the target labels to y.\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42): This splits the data into training and testing sets, with 20% of the data reserved for testing. The random_state parameter ensures that the split is reproducible.\n",
    "\n",
    "pipeline = make_pipeline(StandardScaler(), LogisticRegression(max_iter=500)): This creates a pipeline that first scales the data using StandardScaler and then applies LogisticRegression with a maximum of 500 iterations for convergence.\n",
    "\n",
    "param_grid: This dictionary defines the hyperparameter grid for tuning. It includes:\n",
    "\n",
    "'logisticregression__C': The regularization strength, which can take values from 0.001 to 100.\n",
    "\n",
    "'logisticregression__penalty': The type of regularization, which can be either 'l1' (Lasso) or 'l2' (Ridge).\n",
    "\n",
    "'logisticregression__solver': The algorithm to use for optimization, allowing the use of 'liblinear' or 'saga'.\n",
    "\n",
    "grid_search = GridSearchCV(pipeline, param_grid, cv=5, scoring='accuracy'): This initializes GridSearchCV, which will search for the best hyperparameters by evaluating the model with 5-fold cross-validation and using accuracy as the scoring metric.\n",
    "\n",
    "grid_search.fit(X_train, y_train): This fits the GridSearchCV instance to the training data, running the grid search for hyperparameter optimization.\n",
    "\n",
    "print(\"Best hyperparameters:\", grid_search.best_params_): This prints the best hyperparameters found during the grid search.\n",
    "\n",
    "print(\"Best accuracy:\", grid_search.best_score_): This prints the best accuracy achieved with the optimal hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coefficient shape: (20,)\n",
      "Number of feature names: 20\n",
      "Most impactful features with their coefficients and percentage impact:\n",
      "             Feature  Coefficient  Percentage Impact\n",
      "5               fbs     2.671585          84.793357\n",
      "11               ca     0.266887           8.470714\n",
      "2                cp     0.107302           3.405645\n",
      "10            slope     0.065227           2.070233\n",
      "1               sex     0.000000           0.000000\n",
      "18       feature_18     0.000000           0.000000\n",
      "16       feature_16     0.000000           0.000000\n",
      "15  potassium_level     0.000000           0.000000\n",
      "13     sodium_level     0.000000           0.000000\n",
      "12             thal     0.000000           0.000000\n",
      "0               age     0.000000           0.000000\n",
      "9           oldpeak     0.000000           0.000000\n",
      "8             exang     0.000000           0.000000\n",
      "7           thalach     0.000000           0.000000\n",
      "6           restecg     0.000000           0.000000\n",
      "4              chol     0.000000           0.000000\n",
      "3          trestbps     0.000000           0.000000\n",
      "19       feature_19     0.000000           0.000000\n",
      "17       feature_17    -0.007325           0.232481\n",
      "14  magnesium_level    -0.032376           1.027570\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Get the best model's coefficients from grid search\n",
    "best_model = grid_search.best_estimator_\n",
    "\n",
    "# Retrieve the coefficients\n",
    "coefficients = best_model.named_steps['logisticregression'].coef_[0]\n",
    "\n",
    "# Check the shape of the coefficients\n",
    "print(\"Coefficient shape:\", coefficients.shape)\n",
    "\n",
    "# Manually specify your feature names\n",
    "feature_names = ['age', 'sex', 'cp', 'trestbps', 'chol', \n",
    "                 'fbs', 'restecg', 'thalach', 'exang', \n",
    "                 'oldpeak', 'slope', 'ca', 'thal', \n",
    "                 'sodium_level', 'magnesium_level', 'potassium_level', \n",
    "                 'feature_16', 'feature_17', 'feature_18', 'feature_19']\n",
    "\n",
    "# Check the length of the feature names list\n",
    "print(\"Number of feature names:\", len(feature_names))\n",
    "\n",
    "# Check if the lengths of feature names and coefficients match\n",
    "if len(feature_names) == len(coefficients):\n",
    "    # Create a DataFrame with features and their coefficients\n",
    "    coef_df = pd.DataFrame({'Feature': feature_names, 'Coefficient': coefficients})\n",
    "\n",
    "    # Calculate the percentage impact based on the absolute value of coefficients\n",
    "    coef_df['Percentage Impact'] = (np.abs(coef_df['Coefficient']) / np.sum(np.abs(coefficients))) * 100\n",
    "\n",
    "    # Sort the DataFrame by the coefficients in descending order\n",
    "    coef_df = coef_df.sort_values(by='Coefficient', ascending=False)\n",
    "\n",
    "    # Print the features with their coefficients and percentage impact\n",
    "    print(\"Most impactful features with their coefficients and percentage impact:\\n\", coef_df)\n",
    "else:\n",
    "    print(\"Error: The lengths of feature names and coefficients are not the same.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Coefficients and Percentage Impact\n",
    "\n",
    "Coefficient: The coefficient for each feature represents its influence on the target variable (e.g., risk of heart disease):\n",
    "\n",
    "Positive Coefficient: A positive coefficient indicates that an increase in the feature results in an increase in the target variable. For example, the coefficient for fbs (fasting blood sugar level above 120 mg/dl) is 2.671585, suggesting that having a high fasting blood sugar level increases the risk of heart disease.\n",
    "\n",
    "Negative Coefficient: A negative coefficient implies that an increase in the feature leads to a decrease in the target variable. For instance, the coefficient for magnesium_level is -0.032376, indicating that higher magnesium levels are associated with a reduced risk of heart disease.\n",
    "\n",
    "Zero Coefficient: A coefficient of zero means that the feature has no effect on the target variable. Many features in your results have a coefficient of zero, indicating that they do not influence the risk of heart disease.\n",
    "\n",
    "Percentage Impact: This shows the relative importance of each feature in the modelâ€™s total impact. Features with a high percentage impact play a more critical role in the model's decisions. For example:\n",
    "\n",
    "The feature fbs accounts for 84.79% of the total impact, making it the most significant factor in predicting heart disease risk.\n",
    "\n",
    "The feature ca has a percentage impact of 8.47, indicating it also plays an important role.\n",
    "\n",
    "Many other features (e.g., age, thal, oldpeak, etc.) have zero coefficients, indicating they have no effect in the model.\n",
    "Interpretation of Results\n",
    "\n",
    "Most Impactful Features:\n",
    "\n",
    "fbs (Fasting Blood Sugar): With the highest positive coefficient, this feature suggests that a higher fasting blood sugar level is likely to increase the risk of heart disease. It is an important factor to consider in healthcare.\n",
    "\n",
    "ca (Coronary Angiography): A significant positive coefficient indicates that this condition might also contribute to an increased risk of heart disease.\n",
    "\n",
    "Negative Impact Features:\n",
    "\n",
    "magnesium_level: The negative coefficient indicates that as magnesium levels increase, the risk of heart disease decreases. High magnesium levels may serve as a positive indicator for heart health.\n",
    "\n",
    "Features with No Effect:\n",
    "\n",
    "Several features show a coefficient of zero, meaning these variables do not contribute to the prediction of heart disease risk. This lack of influence suggests that they may not be relevant in the context of this model.\n",
    "\n",
    "Conclusion\n",
    "\n",
    "In summary, the analysis shows that fasting blood sugar level (fbs) and coronary angiography (ca) are critical factors in predicting heart disease risk, while magnesium levels appear to have a protective effect. Many other features do not have a significant influence, which can inform future modeling and feature selection efforts."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
